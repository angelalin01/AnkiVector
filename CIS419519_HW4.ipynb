{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CIS419519_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelalin01/AnkiVector/blob/master/CIS419519_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fQ-AA4aic72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "cb2c12a3-1cdb-4bf7-8a48-05a29c9efb9b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyUMkljX3xM3"
      },
      "source": [
        "# 1 Neural Networks [40 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmpQflsV3cXz"
      },
      "source": [
        "The areas where you need to make changes are marked with *TODO*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjsmBC3V3gS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed2749b-e3b9-45e0-c599-3529b705d551"
      },
      "source": [
        "# Running this cell will download the CIFAR data to the machine that the\n",
        "# notebook is running on. You may need to rerun this every time you open\n",
        "# the notebook\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/train_images.npy\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/train_labels.npy\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/valid_images.npy\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/valid_labels.npy\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/test_images.npy\n",
        "!wget https://www.seas.upenn.edu/~ddeutsch/test_labels.npy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-28 16:28:31--  https://www.seas.upenn.edu/~ddeutsch/train_images.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24576128 (23M)\n",
            "Saving to: ‘train_images.npy’\n",
            "\n",
            "train_images.npy    100%[===================>]  23.44M  24.7MB/s    in 1.0s    \n",
            "\n",
            "2020-11-28 16:28:32 (24.7 MB/s) - ‘train_images.npy’ saved [24576128/24576128]\n",
            "\n",
            "--2020-11-28 16:28:32--  https://www.seas.upenn.edu/~ddeutsch/train_labels.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64128 (63K)\n",
            "Saving to: ‘train_labels.npy’\n",
            "\n",
            "train_labels.npy    100%[===================>]  62.62K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-11-28 16:28:32 (480 KB/s) - ‘train_labels.npy’ saved [64128/64128]\n",
            "\n",
            "--2020-11-28 16:28:33--  https://www.seas.upenn.edu/~ddeutsch/valid_images.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6144128 (5.9M)\n",
            "Saving to: ‘valid_images.npy’\n",
            "\n",
            "valid_images.npy    100%[===================>]   5.86M  10.9MB/s    in 0.5s    \n",
            "\n",
            "2020-11-28 16:28:33 (10.9 MB/s) - ‘valid_images.npy’ saved [6144128/6144128]\n",
            "\n",
            "--2020-11-28 16:28:33--  https://www.seas.upenn.edu/~ddeutsch/valid_labels.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16128 (16K)\n",
            "Saving to: ‘valid_labels.npy’\n",
            "\n",
            "valid_labels.npy    100%[===================>]  15.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-28 16:28:34 (251 MB/s) - ‘valid_labels.npy’ saved [16128/16128]\n",
            "\n",
            "--2020-11-28 16:28:34--  https://www.seas.upenn.edu/~ddeutsch/test_images.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49152128 (47M)\n",
            "Saving to: ‘test_images.npy’\n",
            "\n",
            "test_images.npy     100%[===================>]  46.88M  6.61MB/s    in 6.9s    \n",
            "\n",
            "2020-11-28 16:28:41 (6.84 MB/s) - ‘test_images.npy’ saved [49152128/49152128]\n",
            "\n",
            "--2020-11-28 16:28:41--  https://www.seas.upenn.edu/~ddeutsch/test_labels.npy\n",
            "Resolving www.seas.upenn.edu (www.seas.upenn.edu)... 158.130.68.91, 2607:f470:8:64:5ea5::9\n",
            "Connecting to www.seas.upenn.edu (www.seas.upenn.edu)|158.130.68.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16128 (16K)\n",
            "Saving to: ‘test_labels.npy’\n",
            "\n",
            "test_labels.npy     100%[===================>]  15.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-28 16:28:41 (95.0 MB/s) - ‘test_labels.npy’ saved [16128/16128]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvbRIUu83h1y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN3IXGvG3kpF"
      },
      "source": [
        "# This cell has the code to load the datasets. You should not need\n",
        "# to edit this cell unless you want to do the extra credit. If you do\n",
        "# you should only need to edit normalize_images.\n",
        "class CIFARDataset(Dataset):\n",
        "  def __init__(self, X, y):\n",
        "    self.len = len(X)\n",
        "    self.X = torch.FloatTensor(X).cuda()\n",
        "    self.y = torch.LongTensor(y).cuda()\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def normalize_images(X_train, X_valid, X_test):\n",
        "  \"\"\"\n",
        "  Normalizes the images based on the means and standard deviations\n",
        "  of the training channels. Returns the new normalized images.\n",
        "  \"\"\"\n",
        "  # TODO Implement this method for the extra credit experiments\n",
        "  raise NotImplementedError()\n",
        "    \n",
        "\n",
        "def load_datasets(normalize=False):\n",
        "  X_train = np.load('train_images.npy').astype(float)\n",
        "  y_train = np.load('train_labels.npy')\n",
        "  X_valid = np.load('valid_images.npy').astype(float)\n",
        "  y_valid = np.load('valid_labels.npy')\n",
        "  X_test = np.load('test_images.npy').astype(float)\n",
        "  y_test = np.load('test_labels.npy')\n",
        "\n",
        "  if normalize:\n",
        "    X_train, X_valid, X_test = normalize_images(X_train, X_valid, X_test)\n",
        "  \n",
        "  train_data = CIFARDataset(X_train, y_train)\n",
        "  valid_data = CIFARDataset(X_valid, y_valid)\n",
        "  test_data = CIFARDataset(X_test, y_test)\n",
        "  \n",
        "  return train_data, valid_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K50EdULN3lj7"
      },
      "source": [
        "# This is the implementation of the first network architecture. We have\n",
        "# started it, but you need to finish it. Do not change the class name\n",
        "# or the name of the data members \"fc1\" or \"fc2\"\n",
        "class FeedForward(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = torch.nn.Linear(3072, 1000)\n",
        "    # TODO\n",
        "    # You need to add the second layer's parameters\n",
        "    self.fc2 = torch.nn.Linear(1000, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size = X.size(0)\n",
        "    # This next line reshapes the tensor to be size (B x 3072)\n",
        "    # so it can be passed through a linear layer.\n",
        "    X = X.view(batch_size, -1)\n",
        "    # TODO\n",
        "    # You need to pass X through the two linear layers and relu\n",
        "    # then return the final scores\n",
        "    \n",
        "    X = self.fc1.reshape(X)\n",
        "    X = torch.relu(X)\n",
        "    X = self.fc2.reshape(X)\n",
        "    accuracy, average_loss = compute_loss_and_accuracy(self, X)\n",
        "    return X\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2d0IsbQ3mYE"
      },
      "source": [
        "# This is the implementation of the second network architecture. We have\n",
        "# started it, but you need to finish it. Do not change the class name\n",
        "# or the name of the data members \"conv1\", \"pool\", \"conv2\", \"fc1\", \"fc2\",\n",
        "# or \"fc3\".\n",
        "class Convolutional(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=3,\n",
        "                                 out_channels=7,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    # TODO\n",
        "    # You need to add the pooling, second convolution, and\n",
        "    # three linear modules here\n",
        "    self.pool = torch.nn.max_pool2d(kernel_size=2,stride=2)\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=7,\n",
        "                                 out_channels=16,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "    self.fc1 = torch.nn.Linear(2704, 130)\n",
        "    self.fc2 = torch.nn.Linear(130, 72)\n",
        "    self.fc3 = torch.nn.Linear(72, 10)\n",
        "\n",
        "  def forward(self, X):\n",
        "    batch_size = X.size(0)\n",
        "    X = self.conv1(X)\n",
        "    X = self.pool(X)\n",
        "    X = self.conv2(X)\n",
        "    X = torch.relu(X)\n",
        "    X = self.fc1.reshape(X)\n",
        "    X = torch.relu(X)\n",
        "    X = self.fc2.reshape(X)\n",
        "    X = torch.relu(X)\n",
        "    X = self.fc3.reshape(X)\n",
        "    X = torch.sigmoid(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NucnUvwa3nVK"
      },
      "source": [
        "# You need to finish implementing this method\n",
        "def compute_loss_and_accuracy(network, data_loader):\n",
        "  \"\"\"\n",
        "  Given a network, iterate over the dataset defined by the data_loader\n",
        "  and compute the accuracy of the model and the average loss.\n",
        "  \"\"\"\n",
        "  # This should be used to accumulate the total loss on the dataset\n",
        "  total_loss = 0\n",
        "\n",
        "  # This should count how many examples were correctly classified.\n",
        "  num_correct = 0\n",
        "\n",
        "  # This should count the number of examples in the dataset. (Be careful\n",
        "  # because it should -not- be the number of batches.)\n",
        "  num_instances = 0\n",
        "\n",
        "  # The CrossEntropyLoss by default will return the average loss\n",
        "  # for the batch. So, when you accumulate the total_loss, make sure\n",
        "  # to multiply the loss computed by CrossEntropyLoss by the batch size\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  for X, y in data_loader:\n",
        "    # TODO\n",
        "    # You need to implement computing the loss and\n",
        "    # calculate the number of correct examples.\n",
        "\n",
        "    #calculate correct examples\n",
        "    output = network(X)\n",
        "    num_correct += (output == y).float().sum()\n",
        "\n",
        "    #loss computation\n",
        "    loss = cross_entropy_loss(output, y)\n",
        "    batch_size = X.size(0)\n",
        "    total_loss = total_loss + batch_size * loss\n",
        "    # loss.backward()\n",
        "\n",
        "    # total examples\n",
        "    num_instances = num_instances + 1;\n",
        "  \n",
        "  accuracy = num_correct / num_instances * 100\n",
        "  average_loss = total_loss / num_instances\n",
        "  return accuracy, average_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icU9s0Af3oL3"
      },
      "source": [
        "# You need to finish implementing this method\n",
        "def run_experiment(network, train_data_loader, valid_data_loader, optimizer):\n",
        "  # This will be a list of the average training losses for each epoch\n",
        "  train_losses = []\n",
        "  # This will be a list of the average validation losses for each epoch\n",
        "  valid_accs = []\n",
        "  # This will be a list of the validation accuracies for each epoch\n",
        "  valid_losses = []\n",
        "  # The CrossEntropyLoss by default will return the average loss\n",
        "  # for the batch. So, when you accumulate the total_loss, make sure\n",
        "  # to multiply the loss computed by CrossEntropyLoss by the batch size\n",
        "  cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
        "  for epoch in range(200):\n",
        "    # This should be used to accumulate the total loss on the training data\n",
        "    total_loss = 0.0\n",
        "    # This should be used to count the number of training examples. (Be careful\n",
        "    # because this is not the number of batches)\n",
        "    num_instances = 0\n",
        "    for X, y in train_data_loader:\n",
        "      # TODO\n",
        "      # You need to implement computing the loss for this batch\n",
        "      # and updating the model's parameters.\n",
        "      optimizer.zero_grad()\n",
        "      #calculate correct examples\n",
        "      output = network(X)\n",
        "      num_correct += (output == y).float().sum()\n",
        "\n",
        "      #loss computation\n",
        "      loss = cross_entropy_loss(output, y)\n",
        "      batch_size = X.size(0)\n",
        "      total_loss = total_loss + batch_size * loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # total examples\n",
        "      num_instances = num_instances + 1;\n",
        "\n",
        "    train_loss = total_loss / num_instances\n",
        "    valid_acc, valid_loss = compute_loss_and_accuracy(network, valid_data_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_accs.append(valid_acc)\n",
        "    valid_losses.append(valid_loss)\n",
        "  return train_losses, valid_accs, valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNU23swd3pII"
      },
      "source": [
        "# Load the data and create the iterators. You should not need\n",
        "# to modify this cell\n",
        "train_data, valid_data, test_data = load_datasets(normalize=False)\n",
        "train_data_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "valid_data_loader = DataLoader(valid_data, batch_size=64)\n",
        "test_data_loader = DataLoader(test_data, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knmg286M3qNw"
      },
      "source": [
        "# Implements the FeedForward experiment. You can base the Convolutional experiment\n",
        "# on this code. You should not need to edit this cell.\n",
        "best_network = None\n",
        "best_acc = None\n",
        "\n",
        "_, axs = plt.subplots(1,3)\n",
        "axs[0].set_title('Training Loss')\n",
        "axs[1].set_title('Validation Loss')\n",
        "axs[2].set_title('Validation Accuracies')\n",
        "\n",
        "for lr in [0.0001, 0.00005, 0.00001]:\n",
        "  network = FeedForward()\n",
        "  network.cuda()\n",
        "  sgd = torch.optim.SGD(network.parameters(), lr=lr)\n",
        "\n",
        "  train_losses, valid_accs, valid_losses = run_experiment(network, train_data_loader, valid_data_loader, sgd)\n",
        "  valid_acc = valid_accs[-1]\n",
        "  print(f'LR = {lr}, Valid Acc: {valid_acc}')\n",
        "  if best_acc is None or valid_acc > best_acc:\n",
        "    best_acc = valid_acc\n",
        "    best_network = network\n",
        "\n",
        "  axs[0].plot(train_losses, label=str(lr))\n",
        "  axs[1].plot(valid_losses, label=str(lr))\n",
        "  axs[2].plot(valid_accs, label=str(lr))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "test_acc, _ = compute_loss_and_accuracy(best_network, test_data_loader)\n",
        "print('Test Accuracy: ' + str(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM-SuSNo2mjm"
      },
      "source": [
        "# TODO\n",
        "# You should implement the Convolutional experiment here. It should be\n",
        "# very similar to the cell above.\n",
        "best_network = None\n",
        "best_acc = None\n",
        "\n",
        "_, axs = plt.subplots(1,3)\n",
        "axs[0].set_title('Training Loss')\n",
        "axs[1].set_title('Validation Loss')\n",
        "axs[2].set_title('Validation Accuracies')\n",
        "\n",
        "for lr in [0.01, 0.001, 0.0001]:\n",
        "  network = Convolutional()\n",
        "  network.cuda()\n",
        "  sgd = torch.optim.SGD(network.parameters(), lr=lr)\n",
        "\n",
        "  train_losses, valid_accs, valid_losses = run_experiment(network, train_data_loader, valid_data_loader, sgd)\n",
        "  valid_acc = valid_accs[-1]\n",
        "  print(f'LR = {lr}, Valid Acc: {valid_acc}')\n",
        "  if best_acc is None or valid_acc > best_acc:\n",
        "    best_acc = valid_acc\n",
        "    best_network = network\n",
        "\n",
        "  axs[0].plot(train_losses, label=str(lr))\n",
        "  axs[1].plot(valid_losses, label=str(lr))\n",
        "  axs[2].plot(valid_accs, label=str(lr))\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "test_acc, _ = compute_loss_and_accuracy(best_network, test_data_loader)\n",
        "print('Test Accuracy: ' + str(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUpTY69x8nDg"
      },
      "source": [
        "## (Optional) Extra Credit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74OWiCZb3bI4"
      },
      "source": [
        "# TODO\n",
        "# If you want to run the extra credit experiment, repeat the above experiments\n",
        "# but load the normalized data.\n",
        "train_data, valid_data, test_data = load_datasets(normalize=True)\n",
        "train_data_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "valid_data_loader = DataLoader(valid_data, batch_size=64)\n",
        "test_data_loader = DataLoader(test_data, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYwXnmT84JN8"
      },
      "source": [
        "# 2 Document Classification [40 Points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sraIV0B15sjE"
      },
      "source": [
        "##2.2 Document Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tra_pwaF4MVE"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apJgSHlX4W1x"
      },
      "source": [
        "def get_vocabulary(D):\n",
        "    \"\"\"\n",
        "    Given a list of documents, where each document is represented as\n",
        "    a list of tokens, return the resulting vocabulary. The vocabulary\n",
        "    should be a set of tokens which appear more than once in the entire\n",
        "    document collection plus the \"<unk>\" token.\n",
        "    \"\"\"\n",
        "    vocab = []\n",
        "    vocab_freq = {}\n",
        "    vocab.append(\"<unk>\")\n",
        "    # check frequencies\n",
        "    for doc in D:\n",
        "      for token in doc:\n",
        "        if token in vocab_freq:\n",
        "          vocab_freq[token] += 1\n",
        "        else:\n",
        "          vocab_freq[token] = 1\n",
        "    for token, freq in vocab_freq:\n",
        "      if vocab_freq[token] > 1:\n",
        "        vocab.append(token)\n",
        "          \n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUDce-K64Ybu"
      },
      "source": [
        "class BBoWFeaturizer(object):\n",
        "  def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "    \"\"\"\n",
        "    Given a document represented as a list of tokens and the vocabulary\n",
        "    as a set of tokens, compute the binary bag-of-words feature representation.\n",
        "    This function should return a dictionary which maps from the name of the\n",
        "    feature to the value of that feature.\n",
        "    \"\"\"\n",
        "    feat_dict = {}\n",
        "    for token in doc:\n",
        "      if token in vocab:\n",
        "        feat_dict[token] = 1\n",
        "      else:\n",
        "        feat_dict[\"<unk>\"] = 1\n",
        "    return feat_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRGSMxFp4ZZP"
      },
      "source": [
        "class CBoWFeaturizer(object):\n",
        "  def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "    \"\"\"\n",
        "    Given a document represented as a list of tokens and the vocabulary\n",
        "    as a set of tokens, compute the count bag-of-words feature representation.\n",
        "    This function should return a dictionary which maps from the name of the\n",
        "    feature to the value of that feature.\n",
        "    \"\"\"\n",
        "    feat_dict = {}\n",
        "    for token in doc:\n",
        "      if token in vocab:\n",
        "        if token in feat_dict:\n",
        "          feat_dict[token] += 1\n",
        "        else:\n",
        "          feat_dict[token] = 1\n",
        "      else:\n",
        "        if \"<unk>\" in feat_dict:\n",
        "          feat_dict[\"<unk>\"] += 1\n",
        "        else:\n",
        "          feat_dict[\"<unk>\"] = 1\n",
        "    return feat_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjsIv_JU4aKo"
      },
      "source": [
        "def compute_idf(D, vocab):\n",
        "    \"\"\"\n",
        "    Given a list of documents D and the vocabulary as a set of tokens,\n",
        "    where each document is represented as a list of tokens, return the IDF scores\n",
        "    for every token in the vocab. The IDFs should be represented as a dictionary that\n",
        "    maps from the token to the IDF value. If a token is not present in the\n",
        "    vocab, it should be mapped to \"<unk>\".\n",
        "    \"\"\"\n",
        "    list_size = len(D)\n",
        "    idf_scores = {}\n",
        "    for token in vocab:\n",
        "      doc_count = 0\n",
        "      idf_score = 0\n",
        "      for doc in D:\n",
        "        if token in doc:\n",
        "          doc_count++\n",
        "      idf_score = numpy.log((list_size/doc_count))\n",
        "      idf_scores[token] = idf_score\n",
        "\n",
        "    return idf_scores\n",
        "    \n",
        "class TFIDFFeaturizer(object):\n",
        "    def __init__(self, idf):\n",
        "        \"\"\"The idf scores computed via `compute_idf`.\"\"\"\n",
        "        self.idf = idf\n",
        "    \n",
        "    def convert_document_to_feature_dictionary(self, doc, vocab):\n",
        "        \"\"\"\n",
        "        Given a document represented as a list of tokens and\n",
        "        the vocabulary as a set of tokens, compute\n",
        "        the TF-IDF feature representation. This function\n",
        "        should return a dictionary which maps from the name of the\n",
        "        feature to the value of that feature.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        feat_dict_tf_idf = {}\n",
        "        feat_dict_tf = convert_document_to_feature_dictionary(self, doc, vocab)\n",
        "        for token, tf in feat_dict_tf:\n",
        "          feat_dict_tf_idf[token] = feat_dict_tf[token] * self.idf[token]\n",
        "        return feat_dict_tf_idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roHV8iYq4bKB"
      },
      "source": [
        "# You should not need to edit this cell\n",
        "def load_document_dataset(file_path):\n",
        "    D = []\n",
        "    y = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            instance = json.loads(line)\n",
        "            D.append(instance['document'])\n",
        "            y.append(instance['label'])\n",
        "    return D, y\n",
        "\n",
        "def convert_to_features(D, featurizer, vocab):\n",
        "    X = []\n",
        "    for doc in D:\n",
        "        X.append(featurizer.convert_document_to_feature_dictionary(doc, vocab))\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z7dNapv5zJQ"
      },
      "source": [
        "##2.3 Naive Bayes Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXh1H6vr4b-4"
      },
      "source": [
        "def train_naive_bayes(X, y, k, vocab):\n",
        "    \"\"\"\n",
        "    Computes the statistics for the Naive Bayes classifier.\n",
        "    X is a list of feature representations, where each representation\n",
        "    is a dictionary that maps from the feature name to the value.\n",
        "    y is a list of integers that represent the labels.\n",
        "    k is a float which is the smoothing parameters.\n",
        "    vocab is the set of vocabulary tokens.\n",
        "    \n",
        "    Returns two values:\n",
        "        p_y: A dictionary from the label to the corresponding p(y) score\n",
        "        p_v_y: A nested dictionary where the outer dictionary's key is\n",
        "            the label and the innner dictionary maps from a feature\n",
        "            to the probability p(v|y). For example, `p_v_y[1][\"hello\"]`\n",
        "            should be p(v=\"hello\"|y=1).\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u4XbLrV4cvW"
      },
      "source": [
        "def predict_naive_bayes(D, p_y, p_v_y):\n",
        "    \"\"\"\n",
        "    Runs the prediction rule for Naive Bayes. D is a list of documents,\n",
        "    where each document is a list of tokens.\n",
        "    p_y and p_v_y are output from `train_naive_bayes`.\n",
        "    \n",
        "    Note that any token which is not in p_v_y should be mapped to\n",
        "    \"<unk>\". Further, the input dictionaries are probabilities. You\n",
        "    should convert them to log-probabilities while you compute\n",
        "    the Naive Bayes prediction rule to prevent underflow errors.\n",
        "    \n",
        "    Returns two values:\n",
        "        predictions: A list of integer labels, one for each document,\n",
        "            that is the predicted label for each instance.\n",
        "        confidences: A list of floats, one for each document, that is\n",
        "            p(y|d) for the corresponding label that is returned.\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSRiYOyl8Lxl"
      },
      "source": [
        "## Running experiments for document classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsyhRtua4ekk"
      },
      "source": [
        "# Variables that are named D_* are lists of documents where each\n",
        "# document is a list of tokens. y_* is a list of integer class labels.\n",
        "# X_* is a list of the feature dictionaries for each document.\n",
        "# TODO you likely need to update these paths for your drive setup.\n",
        "D_train, y_train = load_document_dataset('/content/drive/MyDrive/Colab Notebooks/data/train.jsonl')\n",
        "D_valid, y_valid = load_document_dataset('/content/drive/MyDrive/Colab Notebooks/data/valid.jsonl')\n",
        "D_test, y_test = load_document_dataset('/content/drive/MyDrive/Colab Notebooks/data/test.jsonl')\n",
        "\n",
        "vocab = get_vocabulary(D_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ua3cYy34fnv"
      },
      "source": [
        "# Compute the features, for example, using the BBowFeaturizer.\n",
        "# You actually only need to conver the training instances to their\n",
        "# feature-based representations.\n",
        "# \n",
        "# This is just starter code for the experiment. You need to fill in\n",
        "# the rest.\n",
        "featurizer = BBoWFeaturizer()\n",
        "X_train = convert_to_features(D_train, featurizer, vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}